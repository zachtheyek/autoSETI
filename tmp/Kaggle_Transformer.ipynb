{"cells":[{"cell_type":"markdown","metadata":{"id":"YPMOD5n8VL_i"},"source":["# Yuhong: pad input tensors with zeros if model has to take inputs with constant length"]},{"cell_type":"markdown","metadata":{"id":"ofW023h6S3zn"},"source":["# Yuhong: technically we're going for a vision transformer with 1x273 (or 273x1) patches"]},{"cell_type":"markdown","metadata":{},"source":["## Useful links: [https://www.machinecurve.com/index.php/2020/12/28/introduction-to-transformers-in-machine-learning](https://www.machinecurve.com/index.php/2020/12/28/introduction-to-transformers-in-machine-learning), [https://pytorch.org/tutorials/beginner/transformer_tutorial.html](https://pytorch.org/tutorials/beginner/transformer_tutorial.html)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"F184Ko9j_sIk"},"outputs":[],"source":["%matplotlib inline"]},{"cell_type":"markdown","metadata":{"id":"tCjDvPxsE2Qx"},"source":["*   YouTube video explaining Transformers: [https://www.youtube.com/watch?v=TQQlZhbC5ps&list=TLPQMDYwNzIwMjFuBc39xf3IYg&index=9&ab_channel=CodeEmporium](https://www.youtube.com/watch?v=TQQlZhbC5ps&list=TLPQMDYwNzIwMjFuBc39xf3IYg&index=9&ab_channel=CodeEmporium)\n","*   Original Transformers paper: [https://arxiv.org/pdf/1706.03762.pdf](https://arxiv.org/pdf/1706.03762.pdf)"]},{"cell_type":"markdown","metadata":{"id":"CiMakt9HGTiv"},"source":["# Import dependencies"]},{"cell_type":"markdown","metadata":{"id":"yYirmjEnS6OF"},"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":64231,"status":"ok","timestamp":1628181961229,"user":{"displayName":"Xi Yek","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ghsel-R7K7s_q-SvJxQVDOycfUb2Hl6b6qrHiG-Rw=s64","userId":"07143882425028265990"},"user_tz":420},"id":"rvd7G4AX8JuZ","outputId":"422897bd-55e9-41e0-8577-b31b3d8e10bc"},"outputs":[{"name":"stdout","output_type":"stream","text":["Collecting torch-xla==1.9\n","  Downloading https://storage.googleapis.com/tpu-pytorch/wheels/torch_xla-1.9-cp37-cp37m-linux_x86_64.whl (149.9 MB)\n","\u001b[K     |████████████████████████████████| 149.9 MB 50 kB/s \n","\u001b[?25hCollecting cloud-tpu-client==0.10\n","  Downloading cloud_tpu_client-0.10-py3-none-any.whl (7.4 kB)\n","Requirement already satisfied: oauth2client in /usr/local/lib/python3.7/dist-packages (from cloud-tpu-client==0.10) (4.1.3)\n","Collecting google-api-python-client==1.8.0\n","  Downloading google_api_python_client-1.8.0-py3-none-any.whl (57 kB)\n","\u001b[K     |████████████████████████████████| 57 kB 2.3 MB/s \n","\u001b[?25hRequirement already satisfied: httplib2<1dev,>=0.9.2 in /usr/local/lib/python3.7/dist-packages (from google-api-python-client==1.8.0->cloud-tpu-client==0.10) (0.17.4)\n","Requirement already satisfied: google-auth>=1.4.1 in /usr/local/lib/python3.7/dist-packages (from google-api-python-client==1.8.0->cloud-tpu-client==0.10) (1.32.1)\n","Requirement already satisfied: google-api-core<2dev,>=1.13.0 in /usr/local/lib/python3.7/dist-packages (from google-api-python-client==1.8.0->cloud-tpu-client==0.10) (1.26.3)\n","Requirement already satisfied: uritemplate<4dev,>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from google-api-python-client==1.8.0->cloud-tpu-client==0.10) (3.0.1)\n","Requirement already satisfied: six<2dev,>=1.6.1 in /usr/local/lib/python3.7/dist-packages (from google-api-python-client==1.8.0->cloud-tpu-client==0.10) (1.15.0)\n","Requirement already satisfied: google-auth-httplib2>=0.0.3 in /usr/local/lib/python3.7/dist-packages (from google-api-python-client==1.8.0->cloud-tpu-client==0.10) (0.0.4)\n","Requirement already satisfied: googleapis-common-protos<2.0dev,>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from google-api-core<2dev,>=1.13.0->google-api-python-client==1.8.0->cloud-tpu-client==0.10) (1.53.0)\n","Requirement already satisfied: pytz in /usr/local/lib/python3.7/dist-packages (from google-api-core<2dev,>=1.13.0->google-api-python-client==1.8.0->cloud-tpu-client==0.10) (2018.9)\n","Requirement already satisfied: protobuf>=3.12.0 in /usr/local/lib/python3.7/dist-packages (from google-api-core<2dev,>=1.13.0->google-api-python-client==1.8.0->cloud-tpu-client==0.10) (3.17.3)\n","Requirement already satisfied: requests<3.0.0dev,>=2.18.0 in /usr/local/lib/python3.7/dist-packages (from google-api-core<2dev,>=1.13.0->google-api-python-client==1.8.0->cloud-tpu-client==0.10) (2.23.0)\n","Requirement already satisfied: setuptools>=40.3.0 in /usr/local/lib/python3.7/dist-packages (from google-api-core<2dev,>=1.13.0->google-api-python-client==1.8.0->cloud-tpu-client==0.10) (57.2.0)\n","Requirement already satisfied: packaging>=14.3 in /usr/local/lib/python3.7/dist-packages (from google-api-core<2dev,>=1.13.0->google-api-python-client==1.8.0->cloud-tpu-client==0.10) (21.0)\n","Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from google-auth>=1.4.1->google-api-python-client==1.8.0->cloud-tpu-client==0.10) (4.2.2)\n","Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from google-auth>=1.4.1->google-api-python-client==1.8.0->cloud-tpu-client==0.10) (0.2.8)\n","Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.7/dist-packages (from google-auth>=1.4.1->google-api-python-client==1.8.0->cloud-tpu-client==0.10) (4.7.2)\n","Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=14.3->google-api-core<2dev,>=1.13.0->google-api-python-client==1.8.0->cloud-tpu-client==0.10) (2.4.7)\n","Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.7/dist-packages (from pyasn1-modules>=0.2.1->google-auth>=1.4.1->google-api-python-client==1.8.0->cloud-tpu-client==0.10) (0.4.8)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0dev,>=2.18.0->google-api-core<2dev,>=1.13.0->google-api-python-client==1.8.0->cloud-tpu-client==0.10) (2.10)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0dev,>=2.18.0->google-api-core<2dev,>=1.13.0->google-api-python-client==1.8.0->cloud-tpu-client==0.10) (1.24.3)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0dev,>=2.18.0->google-api-core<2dev,>=1.13.0->google-api-python-client==1.8.0->cloud-tpu-client==0.10) (2021.5.30)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0dev,>=2.18.0->google-api-core<2dev,>=1.13.0->google-api-python-client==1.8.0->cloud-tpu-client==0.10) (3.0.4)\n","Installing collected packages: google-api-python-client, torch-xla, cloud-tpu-client\n","  Attempting uninstall: google-api-python-client\n","    Found existing installation: google-api-python-client 1.12.8\n","    Uninstalling google-api-python-client-1.12.8:\n","      Successfully uninstalled google-api-python-client-1.12.8\n","\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n","earthengine-api 0.1.272 requires google-api-python-client<2,>=1.12.1, but you have google-api-python-client 1.8.0 which is incompatible.\u001b[0m\n","Successfully installed cloud-tpu-client-0.10 google-api-python-client-1.8.0 torch-xla-1.9\n"]},{"name":"stderr","output_type":"stream","text":["WARNING:root:Waiting for TPU to be start up with version pytorch-1.9...\n","WARNING:root:Waiting for TPU to be start up with version pytorch-1.9...\n","WARNING:root:TPU has started up successfully with version pytorch-1.9\n"]}],"source":["# Data & storage\n","import os\n","import glob\n","import hashlib\n","from google.colab import drive\n","from torch.utils.data import random_split, DataLoader\n","from torch.utils.data.distributed import DistributedSampler \n","\n","\n","# Analysis\n","import numpy as np\n","import pandas as pd\n","from pandas import read_csv\n","\n","# Visualizations\n","from matplotlib import pyplot as plt\n","from tqdm import tqdm\n","\n","# Deep learning\n","import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","import torch.nn.functional as F\n","\n","# Distributed training (TPUs)\n","!pip install cloud-tpu-client==0.10 https://storage.googleapis.com/tpu-pytorch/wheels/torch_xla-1.9-cp37-cp37m-linux_x86_64.whl\n","import warnings\n","import torch_xla\n","import torch_xla.debug.metrics as met\n","import torch_xla.distributed.data_parallel as dp\n","import torch_xla.distributed.parallel_loader as pl\n","import torch_xla.utils.utils as xu\n","import torch_xla.core.xla_model as xm\n","import torch_xla.distributed.xla_multiprocessing as xmp\n","import torch_xla.test.test_utils as test_utils\n","import warnings\n","warnings.filterwarnings(\"ignore\")\n","\n","# Miscellaneous\n","from typing import Optional, Union"]},{"cell_type":"markdown","metadata":{"id":"U46JvI7eZIRy"},"source":["# Download Data\n","\n"]},{"cell_type":"markdown","metadata":{"id":"VZAD6BgU4_C5"},"source":["To download the Kaggle dataset, we must first mount our Google Drive to this Colab notebook."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":24372,"status":"ok","timestamp":1628181985583,"user":{"displayName":"Xi Yek","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ghsel-R7K7s_q-SvJxQVDOycfUb2Hl6b6qrHiG-Rw=s64","userId":"07143882425028265990"},"user_tz":420},"id":"4JlkraYu3eU_","outputId":"ecd7a16c-9a1e-418c-84fa-9035620c14c1"},"outputs":[{"name":"stdout","output_type":"stream","text":["Mounted at /content/drive\n"]}],"source":["drive.mount('/content/drive')"]},{"cell_type":"markdown","metadata":{"id":"_iCah6A_5Mgu"},"source":["Then, we specify the config path to our Kaggle API token (in the form of a `kaggle.json` file), and change the current working directory to that path."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":788,"status":"ok","timestamp":1628181986364,"user":{"displayName":"Xi Yek","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ghsel-R7K7s_q-SvJxQVDOycfUb2Hl6b6qrHiG-Rw=s64","userId":"07143882425028265990"},"user_tz":420},"id":"bVzYMI8C3qX3","outputId":"936bb8c7-b567-4e47-808a-85e53471f22d"},"outputs":[{"name":"stdout","output_type":"stream","text":["/content/drive/MyDrive/Research/Transformers/Code/Data\n"]}],"source":["os.environ['KAGGLE_CONFIG_DIR'] = '/content/drive/MyDrive/Research/Dynamic Spectra Sequence Modeling/Data/Kaggle'\n","%cd '/content/drive/MyDrive/Research/Ongoing/Dynamic Spectra Sequence Modeling/Data'"]},{"cell_type":"markdown","metadata":{"id":"OUrCQWsk5e3z"},"source":["Finally, we copy and run the API command for the BL Kaggle competition to download the datasets (remember to unzip the files)."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"DVr3S-zQ4cZL"},"outputs":[],"source":["if not os.listdir():\n","  # Note, if you're getting the error message \"429 - Too Many Requests\", try running the following commands before the API command:\n","  # !pip uninstall -y kaggle\n","  # !pip install --upgrade pip\n","  # !pip install kaggle==1.5.6\n","  !kaggle competitions download -c seti-breakthrough-listen\n","  \n","  file_to_extract = 'seti-breakthrough-listen.zip'\n","  # Read in zip file\n","    with ZipFile(file_to_extract,'r') as zip_ref:\n","      # Add progress bar\n","      for file in tqdm(iterable=zip_ref.namelist(), total=len(zip_ref.namelist())):\n","        # Extract and store in current directory\n","        zip_ref.extract(member=file)"]},{"cell_type":"markdown","metadata":{"id":"Cm-0buX-OPfS"},"source":["# Prep Data"]},{"cell_type":"markdown","metadata":{"id":"LNjADlBANJKt"},"source":["We want to create lookup tables in the form of Python dictionaries, with ID-target key-value pairs, for both the training and test data. \n","\n","To do so for the training data is quite straightforward. Note however, that the test data IDs have been hashed for security purposes, hence we must go through some extra steps beforehand."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":116721,"status":"ok","timestamp":1628182103389,"user":{"displayName":"Xi Yek","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ghsel-R7K7s_q-SvJxQVDOycfUb2Hl6b6qrHiG-Rw=s64","userId":"07143882425028265990"},"user_tz":420},"id":"LwbuZ_HIMAEH","outputId":"48103229-7a15-4d93-d789-c411ba7e894f"},"outputs":[{"name":"stdout","output_type":"stream","text":["Enter keyword: zach\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 39995/39995 [01:51<00:00, 359.27it/s]\n"]}],"source":["train_labels = read_csv('train_labels.csv')\n","train_dict = dict(zip(train_labels.id, train_labels.target))\n","\n","original_labels = read_csv('sample_submission.csv')['id']\n","hash_labels = read_csv('masked_labels.csv')\n","test_dict = {}\n","keyword = input('Enter keyword: ')\n","for labels in tqdm(original_labels):\n","  m = hashlib.md5(keyword.encode(\"utf-8\"))\n","  m.update(bytes.fromhex(\"0\" + labels))\n","  hashed_id = m.hexdigest()\n","  test_dict[labels] = hash_labels.loc[hash_labels['id'] == hashed_id, 'target'].item()"]},{"cell_type":"markdown","metadata":{"id":"94bgHuHh2fJA"},"source":["Split the training set into non-overlapping new datasets for cross-validation. Note that `x_train` and `x_valid` will hold the ID values, whereas `y_train` and `y_valid` will hold the target values (both with lengths `(48000, 12000)`, respectively). Since our model is self-supervised, we'll only use `y_train` and `y_valid` for validation using downstream tasks."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ImXsAW2U5Ogn"},"outputs":[],"source":["len_train = int(len(train_labels) * 0.8)\n","len_valid = int(len(train_labels) * 0.2)\n","\n","x_train, x_valid = random_split(train_labels['id'], (len_train, len_valid))\n","y_train, y_valid = random_split(train_labels['target'], (len_train, len_valid))"]},{"cell_type":"markdown","metadata":{"id":"7FLlVOOuOW5O"},"source":["# Class Definitions"]},{"cell_type":"markdown","metadata":{"id":"dap87omPqQNy"},"source":["## Positional Encoding"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"k2tVWd5tpV0l"},"outputs":[],"source":["def positional_encoding(length: int, d_model: int) -> torch.Tensor:\n","    \"\"\"\n","    Generate positional encoding as described in original paper.  :class:`torch.Tensor`\n","    Parameters\n","    ----------\n","    length:\n","        Time window length, i.e. K.\n","    d_model:\n","        Dimension of the model vector.\n","    Returns\n","    -------\n","        Tensor of shape (K, d_model).\n","    \"\"\"\n","    PE = torch.zeros((length, d_model))\n","    pos = torch.arange(length).unsqueeze(1)\n","\n","    PE[:, 0::2] = torch.sin(\n","        pos / torch.pow(1000, torch.arange(0, d_model, 2, dtype=torch.float32)/d_model))\n","    PE[:, 1::2] = torch.cos(\n","        pos / torch.pow(1000, torch.arange(1, d_model, 2, dtype=torch.float32)/d_model))\n","    \n","    return PE"]},{"cell_type":"markdown","metadata":{"id":"vm0-Sq1xqSZd"},"source":["## Masks"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"81qBUfHpqbJk"},"outputs":[],"source":["def generate_local_masks(chunk_size: int,\n","                         attention_size: int,\n","                         mask_future=False,\n","                         device: torch.device = 'cpu') -> torch.BoolTensor:\n","    \"\"\"\n","    Compute attention mask as attention_size wide diagonal.\n","    Parameters\n","    ----------\n","    chunk_size:\n","        Time dimension size.\n","    attention_size:\n","        Number of backward elements to apply attention.\n","    device:\n","        torch device. Default is ``'cpu'``.\n","    Returns\n","    -------\n","        Mask as a boolean tensor.\n","    \"\"\"\n","    local_map = np.empty((chunk_size, chunk_size))\n","    i, j = np.indices(local_map.shape)\n","\n","    if mask_future:\n","        local_map[i, j] = (i - j > attention_size) ^ (j - i > 0)\n","    else:\n","        local_map[i, j] = np.abs(i - j) > attention_size\n","\n","    return torch.BoolTensor(local_map).to(device)"]},{"cell_type":"markdown","metadata":{"id":"ywEQgymVrJ2L"},"source":["## Multi-Headed Attention"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"rt8cPkV-sjcP"},"outputs":[],"source":["class MultiHeadAttention(nn.Module):\n","    \"\"\"\n","    Multi Head Attention block from Attention is All You Need.\n","    Given 3 inputs of shape (batch_size, K, d_model), that will be used\n","    to compute query, keys and values, we output a self attention\n","    tensor of shape (batch_size, K, d_model).\n","    Parameters\n","    ----------\n","    d_model:\n","        Dimension of the input vector.\n","    q:\n","        Dimension of all query matrix.\n","    v:\n","        Dimension of all value matrix.\n","    h:\n","        Number of heads.\n","    attention_size:\n","        Number of backward elements to apply attention.\n","        Deactivated if ``None``. Default is ``None``.\n","    \"\"\"\n","    def __init__(self,\n","                 d_model: int,\n","                 q: int,\n","                 v: int,\n","                 h: int,\n","                 attention_size: int = None):\n","        \"\"\"Initialize the Multi Head Block.\"\"\"\n","        super().__init__()\n","\n","        self._h = h\n","        self._attention_size = attention_size\n","\n","        # Query, keys and value matrices\n","        self._W_q = nn.Linear(d_model, q*self._h)\n","        self._W_k = nn.Linear(d_model, q*self._h)\n","        self._W_v = nn.Linear(d_model, v*self._h)\n","\n","        # Output linear function\n","        self._W_o = nn.Linear(self._h*v, d_model)\n","\n","        # Score placeholder\n","        self._scores = None\n","\n","    def forward(self,\n","                query: torch.Tensor,\n","                key: torch.Tensor,\n","                value: torch.Tensor,\n","                mask: Optional[str] = None) -> torch.Tensor:\n","        \"\"\"\n","        Propagate forward the input through the MHB.\n","        We compute for each head the queries, keys and values matrices,\n","        followed by the Scaled Dot-Product. The result is concatenated \n","        and returned with shape (batch_size, K, d_model).\n","        Parameters\n","        ----------\n","        query:\n","            Input tensor with shape (batch_size, K, d_model) used to compute queries.\n","        key:\n","            Input tensor with shape (batch_size, K, d_model) used to compute keys.\n","        value:\n","            Input tensor with shape (batch_size, K, d_model) used to compute values.\n","        mask:\n","            Mask to apply on scores before computing attention.\n","            One of ``'subsequent'``, None. Default is None.\n","        Returns\n","        -------\n","            Self attention tensor with shape (batch_size, K, d_model).\n","        \"\"\"\n","        K = query.shape[1]\n","\n","        # Compute Q, K and V, concatenate heads on batch dimension\n","        queries = torch.cat(self._W_q(query).chunk(self._h, dim=-1), dim=0)\n","        keys = torch.cat(self._W_k(key).chunk(self._h, dim=-1), dim=0)\n","        values = torch.cat(self._W_v(value).chunk(self._h, dim=-1), dim=0)\n","\n","        # Scaled Dot Product\n","        self._scores = torch.bmm(queries, keys.transpose(1, 2)) / np.sqrt(K)\n","\n","        # Compute local map mask\n","        if self._attention_size is not None:\n","            attention_mask = generate_local_map_mask(K, self._attention_size, mask_future=False, device=self._scores.device)\n","            self._scores = self._scores.masked_fill(attention_mask, float('-inf'))\n","\n","        # Compute future mask\n","        if mask == \"subsequent\":\n","            future_mask = torch.triu(torch.ones((K, K)), diagonal=1).bool()\n","            future_mask = future_mask.to(self._scores.device)\n","            self._scores = self._scores.masked_fill(future_mask, float('-inf'))\n","\n","        # Apply sotfmax\n","        self._scores = F.softmax(self._scores, dim=-1)\n","\n","        attention = torch.bmm(self._scores, values)\n","\n","        # Concatenat the heads\n","        attention_heads = torch.cat(attention.chunk(self._h, dim=0), dim=-1)\n","\n","        # Apply linear transformation W^O\n","        self_attention = self._W_o(attention_heads)\n","\n","        return self_attention\n","\n","    @property\n","    def attention_map(self) -> torch.Tensor:\n","        \"\"\"\n","        Attention map after a forward propagation,\n","        variable `score` in the original paper.\n","        \"\"\"\n","        if self._scores is None:\n","            raise RuntimeError(\n","                \"Evaluate the model once to generate attention map\")\n","        return self._scores"]},{"cell_type":"markdown","metadata":{"id":"wjdIzpP3unka"},"source":["## Feed-Forward Network"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"vTR1vxStuqmZ"},"outputs":[],"source":["class PositionwiseFeedForward(nn.Module):\n","    \"\"\"\n","    Position-wise Feed Forward Network block from Attention is All You Need.\n","    Apply two linear transformations to each input, separately but indetically. We\n","    implement them as 1D convolutions. Input and output have a shape (batch_size, d_model).\n","    Parameters\n","    ----------\n","    d_model:\n","        Dimension of input tensor.\n","    d_ff:\n","        Dimension of hidden layer, default is 2048.\n","    \"\"\"\n","    def __init__(self,\n","                 d_model: int,\n","                 d_ff: Optional[int] = 2048):\n","        \"\"\"Initialize the PFF block.\"\"\"\n","        super().__init__()\n","\n","        self._linear1 = nn.Linear(d_model, d_ff)\n","        self._linear2 = nn.Linear(d_ff, d_model)\n","\n","    def forward(self, x: torch.Tensor) -> torch.Tensor:\n","        \"\"\"Propagate forward the input through the PFF block.\n","        Apply the first linear transformation, then a relu actvation,\n","        and the second linear transformation.\n","        Parameters\n","        ----------\n","        x:\n","            Input tensor with shape (batch_size, K, d_model).\n","        Returns\n","        -------\n","            Output tensor with shape (batch_size, K, d_model).\n","        \"\"\"\n","        return self._linear2(F.relu(self._linear1(x)))"]},{"cell_type":"markdown","metadata":{"id":"N6DcDOMIMYjZ"},"source":["## Loss\n","\n","[SET LOSS FUNCTION SUCH THAT IT CALCULATES LOSS BETWEEN PREDICTED AND ACTUAL SPECTRA FOR NEXT TIMESTEP]"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"divdWnCTMZMj"},"outputs":[],"source":["class OZELoss(nn.Module):\n","    \"\"\"Custom loss for TRNSys metamodel.\n","    Compute, for temperature and consumptions, the intergral of the squared differences\n","    over time. Sum the log with a coeficient ``alpha``.\n","    .. math::\n","        \\Delta_T = \\sqrt{\\int (y_{est}^T - y^T)^2}\n","        \\Delta_Q = \\sqrt{\\int (y_{est}^Q - y^Q)^2}\n","        loss = log(1 + \\Delta_T) + \\\\alpha \\cdot log(1 + \\Delta_Q)\n","    Parameters:\n","    -----------\n","    alpha:\n","        Coefficient for consumption. Default is ``0.3``.\n","    \"\"\"\n","    def __init__(self, reduction: str = 'mean', alpha: float = 0.3):\n","        super().__init__()\n","\n","        self.alpha = alpha\n","        self.reduction = reduction\n","\n","        self.base_loss = nn.MSELoss(reduction=self.reduction)\n","\n","    def forward(self,\n","                y_true: torch.Tensor,\n","                y_pred: torch.Tensor) -> torch.Tensor:\n","        \"\"\"Compute the loss between a target value and a prediction.\n","        Parameters\n","        ----------\n","        y_true:\n","            Target value.\n","        y_pred:\n","            Estimated value.\n","        Returns\n","        -------\n","        Loss as a tensor with gradient attached.\n","        \"\"\"\n","        delta_Q = self.base_loss(y_pred[..., :-1], y_true[..., :-1])\n","        delta_T = self.base_loss(y_pred[..., -1], y_true[..., -1])\n","\n","        if self.reduction == 'none':\n","            delta_Q = delta_Q.mean(dim=(1, 2))\n","            delta_T = delta_T.mean(dim=(1))\n","\n","        return torch.log(1 + delta_T) + self.alpha * torch.log(1 + delta_Q)"]},{"cell_type":"markdown","metadata":{"id":"yaHCDl67MNXl"},"source":["## Decoder"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"0yvKHjpZMOJT"},"outputs":[],"source":["class Decoder(nn.Module):\n","    \"\"\"\n","    Decoder block from Attention is All You Need.\n","    Apply two Multi Head Attention block followed by a Point-wise Feed Forward block.\n","    Residual sum and normalization are applied at each step.\n","    Parameters\n","    ----------\n","    d_model: \n","        Dimension of the input vector.\n","    q:\n","        Dimension of all query matrix.\n","    v:\n","        Dimension of all value matrix.\n","    h:\n","        Number of heads.\n","    attention_size:\n","        Number of backward elements to apply attention.\n","        Deactivated if ``None``. Default is ``None``.\n","    dropout:\n","        Dropout probability after each MHA or PFF block.\n","        Default is ``0.3``.\n","    chunk_mode:\n","        Swict between different MultiHeadAttention blocks.\n","        One of ``'chunk'``, ``'window'`` or ``None``. Default is ``'chunk'``.\n","    \"\"\"\n","    def __init__(self,\n","                 d_model: int,\n","                 q: int,\n","                 v: int,\n","                 h: int,\n","                 attention_size: int = None,\n","                 dropout: float = 0.3,\n","                 chunk_mode: str = 'chunk'):\n","        \"\"\"Initialize the Decoder block\"\"\"\n","        super().__init__()\n","\n","        chunk_mode_modules = {\n","            'chunk': MultiHeadAttentionChunk,\n","            'window': MultiHeadAttentionWindow,\n","        }\n","\n","        if chunk_mode in chunk_mode_modules.keys():\n","            MHA = chunk_mode_modules[chunk_mode]\n","        elif chunk_mode is None:\n","            MHA = MultiHeadAttention\n","        else:\n","            raise NameError(\n","                f'chunk_mode \"{chunk_mode}\" not understood. Must be one of {\", \".join(chunk_mode_modules.keys())} or None.')\n","\n","        self._selfAttention = MHA(d_model, q, v, h, attention_size=attention_size)\n","        self._encoderDecoderAttention = MHA(d_model, q, v, h, attention_size=attention_size)\n","        self._feedForward = PositionwiseFeedForward(d_model)\n","\n","        self._layerNorm1 = nn.LayerNorm(d_model)\n","        self._layerNorm2 = nn.LayerNorm(d_model)\n","        self._layerNorm3 = nn.LayerNorm(d_model)\n","\n","        self._dopout = nn.Dropout(p=dropout)\n","\n","    def forward(self, x: torch.Tensor, memory: torch.Tensor) -> torch.Tensor:\n","        \"\"\"Propagate the input through the Decoder block.\n","        Apply the self attention block, add residual and normalize.\n","        Apply the encoder-decoder attention block, add residual and normalize.\n","        Apply the feed forward network, add residual and normalize.\n","        Parameters\n","        ----------\n","        x:\n","            Input tensor with shape (batch_size, K, d_model).\n","        memory:\n","            Memory tensor with shape (batch_size, K, d_model)\n","            from encoder output.\n","        Returns\n","        -------\n","        x:\n","            Output tensor with shape (batch_size, K, d_model).\n","        \"\"\"\n","        # Self attention\n","        residual = x\n","        x = self._selfAttention(query=x, key=x, value=x, mask=\"subsequent\")\n","        x = self._dopout(x)\n","        x = self._layerNorm1(x + residual)\n","\n","        # Encoder-decoder attention\n","        residual = x\n","        x = self._encoderDecoderAttention(query=x, key=memory, value=memory)\n","        x = self._dopout(x)\n","        x = self._layerNorm2(x + residual)\n","\n","        # Feed forward\n","        residual = x\n","        x = self._feedForward(x)\n","        x = self._dopout(x)\n","        x = self._layerNorm3(x + residual)\n","\n","        return x"]},{"cell_type":"markdown","metadata":{"id":"Qh07xvWHGXQC"},"source":["## Transformer"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"qlXx69lHMHen"},"outputs":[],"source":["class Transformer(nn.Module):\n","    \"\"\"\n","    Transformer model from Attention is All You Need.\n","    A classic transformer model adapted for sequential data.\n","    Embedding has been replaced with a fully connected layer,\n","    the last layer softmax is now a sigmoid.\n","    Attributes\n","    ----------\n","    layers_encoding: :py:class:`list` of :class:`Encoder.Encoder`\n","        stack of Encoder layers.\n","    layers_decoding: :py:class:`list` of :class:`Decoder.Decoder`\n","        stack of Decoder layers.\n","    Parameters\n","    ----------\n","    d_input:\n","        Model input dimension.\n","    d_model:\n","        Dimension of the input vector.\n","    d_output:\n","        Model output dimension.\n","    q:\n","        Dimension of queries and keys.\n","    v:\n","        Dimension of values.\n","    h:\n","        Number of heads.\n","    N:\n","        Number of encoder and decoder layers to stack.\n","    attention_size:\n","        Number of backward elements to apply attention.\n","        Deactivated if ``None``. Default is ``None``.\n","    dropout:\n","        Dropout probability after each MHA or PFF block.\n","        Default is ``0.3``.\n","    chunk_mode:\n","        Switch between different MultiHeadAttention blocks.\n","        One of ``'chunk'``, ``'window'`` or ``None``. Default is ``'chunk'``.\n","    pe:\n","        Type of positional encoding to add.\n","        Must be one of ``'original'``, ``'regular'`` or ``None``. Default is ``None``.\n","    pe_period:\n","        If using the ``'regular'` pe, then we can define the period. Default is ``24``.\n","    \"\"\"\n","    def __init__(self,\n","                 d_input: int,\n","                 d_model: int,\n","                 d_output: int,\n","                 q: int,\n","                 v: int,\n","                 h: int,\n","                 N: int,\n","                 attention_size: int = None,\n","                 dropout: float = 0.3,\n","                 chunk_mode: str = 'chunk',\n","                 pe: str = None,\n","                 pe_period: int = 24):\n","        \"\"\"Create transformer structure from Encoder and Decoder blocks.\"\"\"\n","        super().__init__()\n","\n","        self._d_model = d_model\n","\n","        self.layers_encoding = nn.ModuleList([Encoder(d_model,\n","                                                      q,\n","                                                      v,\n","                                                      h,\n","                                                      attention_size=attention_size,\n","                                                      dropout=dropout,\n","                                                      chunk_mode=chunk_mode) for _ in range(N)])\n","        self.layers_decoding = nn.ModuleList([Decoder(d_model,\n","                                                      q,\n","                                                      v,\n","                                                      h,\n","                                                      attention_size=attention_size,\n","                                                      dropout=dropout,\n","                                                      chunk_mode=chunk_mode) for _ in range(N)])\n","\n","        self._embedding = nn.Linear(d_input, d_model)\n","        self._linear = nn.Linear(d_model, d_output)\n","\n","        pe_functions = {\n","            'original': generate_original_PE,\n","            'regular': generate_regular_PE,\n","        }\n","\n","        if pe in pe_functions.keys():\n","            self._generate_PE = pe_functions[pe]\n","            self._pe_period = pe_period\n","        elif pe is None:\n","            self._generate_PE = None\n","        else:\n","            raise NameError(\n","                f'PE \"{pe}\" not understood. Must be one of {\", \".join(pe_functions.keys())} or None.')\n","\n","        self.name = 'transformer'\n","\n","    def forward(self, x: torch.Tensor) -> torch.Tensor:\n","        \"\"\"Propagate input through transformer\n","        Forward input through an embedding module,\n","        the encoder then decoder stacks, and an output module.\n","        Parameters\n","        ----------\n","        x:\n","            :class:`torch.Tensor` of shape (batch_size, K, d_input).\n","        Returns\n","        -------\n","            Output tensor with shape (batch_size, K, d_output).\n","        \"\"\"\n","        K = x.shape[1]\n","\n","        # Embeddin module\n","        encoding = self._embedding(x)\n","\n","        # Add position encoding\n","        if self._generate_PE is not None:\n","            pe_params = {'period': self._pe_period} if self._pe_period else {}\n","            positional_encoding = self._generate_PE(K, self._d_model, **pe_params)\n","            positional_encoding = positional_encoding.to(encoding.device)\n","            encoding.add_(positional_encoding)\n","\n","        # Encoding stack\n","        for layer in self.layers_encoding:\n","            encoding = layer(encoding)\n","\n","        # Decoding stack\n","        decoding = encoding\n","\n","        # Add position encoding\n","        if self._generate_PE is not None:\n","            positional_encoding = self._generate_PE(K, self._d_model)\n","            positional_encoding = positional_encoding.to(decoding.device)\n","            decoding.add_(positional_encoding)\n","\n","        for layer in self.layers_decoding:\n","            decoding = layer(decoding, encoding)\n","\n","        # Output module\n","        output = self._linear(decoding)\n","        output = torch.sigmoid(output)\n","        return output"]},{"cell_type":"markdown","metadata":{"id":"ef0v8XnaMHqD"},"source":["# Train"]},{"cell_type":"markdown","metadata":{"id":"wgrulI_I1cMu"},"source":["Initialize the random seed."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"d8t1knbZ1bf5"},"outputs":[],"source":["# Random Seed Initialize\n","RANDOM_SEED = 11\n","def seed_everything(seed=RANDOM_SEED):\n","    os.environ['PYTHONHASHSEED'] = str(seed)\n","    np.random.seed(seed)\n","    np.random.seed(seed)\n","    torch.manual_seed(seed)\n","    torch.cuda.manual_seed(seed)\n","    torch.backends.cudnn.deterministic = True\n","    torch.backends.cudnn.benchmark = True\n","seed_everything()"]},{"cell_type":"markdown","metadata":{"id":"8VZ2xfHx9ms2"},"source":["Set the model parameters."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":232},"collapsed":true,"executionInfo":{"elapsed":11,"status":"error","timestamp":1628183262922,"user":{"displayName":"Xi Yek","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ghsel-R7K7s_q-SvJxQVDOycfUb2Hl6b6qrHiG-Rw=s64","userId":"07143882425028265990"},"user_tz":420},"id":"I8H8YOUr9n-y","outputId":"0b86d898-b634-4955-c30c-1e05089fdaf9"},"outputs":[{"ename":"NameError","evalue":"ignored","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-1-7e936f49be0a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m training_params = {\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0;34m'device'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mxm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mxla_device\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m     \u001b[0;34m'lr'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;36m1e-4\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0;34m'batch_size'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;36m1024\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;31m# 128 * 8 - batch size of 128 for each of the 8 TPU cores (to avoid bottlenecking)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0;34m'epochs'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;36m30\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mNameError\u001b[0m: name 'xm' is not defined"]}],"source":["checkpoint_path = '../Code/Checkpoints/.'\n","num_cores = 8\n","num_workers = 0\n","epochs = 30\n","batch_size = 128\n","learning_rate = 1e-4\n","\n","\n","d_model = 256   # Latent dim\n","d_input = (6, t, 256)   # Input dim (from dataset), where 0<= t <= 6*273\n","d_output = (6, 1, 256)   # Output dim (from dataset)\n","q = 8   # Query size\n","v = 8   # Value size\n","h = 8   # Number of heads\n","N = 4   # Number of decoder blocks to stack\n","attention_size = 12   # Attention window size\n","dropout = 0.2   # Dropout rate\n","pe = None\n","chunk_mode = None\n","\n","# training_params = {\n","#     'checkpoint_path': '../Checkpoints/.',\n","#     'num_cores': 8,\n","#     'num_workers': 0,\n","#     'epochs': 30,\n","#     'batch_size': 128,\n","#     'learning_rate': 1e-4\n","# }\n","\n","# # Dimensions for data are (6, 273, 256), i.e. 6 snippets of 273 timesteps and 256 frequency channels\n","# model_params = {\n","#     'd_model': 256, # Latent dim\n","#     'd_input': (6, t, 256), # Input dim (from dataset), where 0<= t <= 6*273\n","#     'd_output': (6, 1, 256), # Output dim (from dataset)\n","#     'q': 8, # Query size\n","#     'v': 8, # Value size\n","#     'h': 8, # Number of heads\n","#     'N': 4, # Number of decoder blocks to stack\n","#     'attention_size': 12, # Attention window size\n","#     'dropout': 0.2, # Dropout rate\n","#     'pe': None,\n","#     'chunk_mode': None\n","# }"]},{"cell_type":"markdown","metadata":{"id":"THxIEOk7KhFw"},"source":["## Configuring Colab's Cloud TPUs\n","\n"]},{"cell_type":"markdown","metadata":{"id":"Hiys2kfbXulk"},"source":["Colab provides a free Cloud TPU system (a remote CPU host + four TPU chips with two cores each). To gain access to a TPU on Colab, on the main menu, click Runtime > Change runtime type > set \"TPU\" as the hardware accelerator.\n","\n","The PyTorch/XLA package lets PyTorch connect to Cloud TPUs (It's named PyTorch/XLA, not PyTorch/TPU, because XLA is the name of the TPU compiler), and makes TPU cores available as PyTorch devices, which lets PyTorch create and manipulate tensors on TPUs."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"vTOMNye1KjUb"},"outputs":[],"source":["assert os.environ['COLAB_TPU_ADDR'], 'Make sure to select TPU from Edit > Notebook settings > Hardware accelerator'"]},{"cell_type":"markdown","metadata":{"id":"GqCXmZ6d-7C3"},"source":["`torch.utils.data.distributed.DistrubutedSampler()` distributes the training data evenly (with no replicas) to all 8 TPU cores that Colab provides. Note that `xm.xrt_world_size()` retrieves the number of devices that are taking part in the replication (basically the number of cores), and `xm.get_ordinal()` retrieves the replication ordinal of the current process. The ordinals range from `0` to `xrt_world_size()-1`."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Xt2MK5_E_O0w"},"outputs":[],"source":["train_sampler = DistributedSampler(\n","    x_train,\n","    num_replicas=xm.xrt_world_size(),\n","    rank=xm.get_ordinal(),\n","    shuffle=True)\n","     \n","valid_sampler = DistributedSampler(\n","    x_valid,\n","    num_replicas=xm.xrt_world_size(),\n","    rank=xm.get_ordinal(),\n","    shuffle=False)"]},{"cell_type":"markdown","metadata":{"id":"bAggrSznGj8B"},"source":["After the data has been distributed, we can create dataloaders using `ParallelLoader`."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"z0PmQAFsG5pw"},"outputs":[],"source":["train_loader = DataLoader(\n","    x_train,\n","    batch_size=training_params['batch_size'],\n","    sampler=train_sampler,\n","    num_workers=training_params['num_workers'],\n","    shuffle=True,\n","    drop_last=True)\n","\n","valid_loader = DataLoader(\n","    x_valid,\n","    batch_size=training_params['batch_size'],\n","    sampler=train_sampler,\n","    num_workers=training_params['num_workers'],\n","    shuffle=False,\n","    drop_last=True)\n","\n","# drop_last = True drops the last incomplete batch if the dataset size is not divisible by the batch size\n","# drop_last = False will cause the last batch to be smaller if the size of dataset is not divisible by the batch size"]},{"cell_type":"markdown","metadata":{"id":"Mrtz5gFCojZi"},"source":["Optimize parameters for distributed training on TPU cores (remember `xm.xrt_world_size()` returns the number of TPU cores, which for our case is 8)."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"X5qTozz1omvK"},"outputs":[],"source":["# Scale learning rate to world size\n","lr = training_params['learning_rate'] * xm.xrt_world_size()\n","\n","# Get loss function, optimizer, and model\n","device = xm.xla_device()\n","model = Transformer().to(device)\n","optimizer = optim.Adam(model.parameters(), lr=lr)\n","loss_function = OZELoss(alpha=0.3)"]},{"cell_type":"markdown","metadata":{"id":"EygLIhmH5rP6"},"source":["## Loop"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"fCZd2HcK5ud8"},"outputs":[],"source":[]}],"metadata":{"accelerator":"TPU","colab":{"collapsed_sections":[],"machine_shape":"hm","name":"Kaggle_transformer.ipynb","provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}
