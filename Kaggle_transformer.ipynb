{
  "nbformat": 4,
  "nbformat_minor": 2,
  "metadata": {
    "colab": {
      "name": "Kaggle_transformer.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "qMvUS1ZsZIFR",
        "vzNNACBlZDo7"
      ],
      "toc_visible": true,
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "%matplotlib inline"
      ],
      "outputs": [],
      "metadata": {
        "id": "F184Ko9j_sIk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "*   YouTube video explaining Transformers: [https://www.youtube.com/watch?v=TQQlZhbC5ps&list=TLPQMDYwNzIwMjFuBc39xf3IYg&index=9&ab_channel=CodeEmporium](https://www.youtube.com/watch?v=TQQlZhbC5ps&list=TLPQMDYwNzIwMjFuBc39xf3IYg&index=9&ab_channel=CodeEmporium)\n",
        "*   Original Transformers paper: [https://arxiv.org/pdf/1706.03762.pdf](https://arxiv.org/pdf/1706.03762.pdf)"
      ],
      "metadata": {
        "id": "tCjDvPxsE2Qx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Import dependencies"
      ],
      "metadata": {
        "id": "CiMakt9HGTiv"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "# Data & storage\n",
        "import os\n",
        "import glob\n",
        "import hashlib\n",
        "from google.colab import drive\n",
        "from torch.utils.data import random_split, DataLoader\n",
        "from torch.utils.data.distributed import DistributedSampler \n",
        "\n",
        "\n",
        "# Analysis\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from pandas import read_csv\n",
        "\n",
        "# Visualizations\n",
        "from matplotlib import pyplot as plt\n",
        "from tqdm import tqdm\n",
        "\n",
        "# Deep learning\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "\n",
        "# Distributed training (TPUs)\n",
        "!pip install cloud-tpu-client==0.10 https://storage.googleapis.com/tpu-pytorch/wheels/torch_xla-1.9-cp37-cp37m-linux_x86_64.whl\n",
        "import warnings\n",
        "import torch_xla\n",
        "import torch_xla.debug.metrics as met\n",
        "import torch_xla.distributed.data_parallel as dp\n",
        "import torch_xla.distributed.parallel_loader as pl\n",
        "import torch_xla.utils.utils as xu\n",
        "import torch_xla.core.xla_model as xm\n",
        "import torch_xla.distributed.xla_multiprocessing as xmp\n",
        "import torch_xla.test.test_utils as test_utils\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "# Miscellaneous\n",
        "from typing import Optional, Union"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting torch-xla==1.9\n",
            "  Downloading https://storage.googleapis.com/tpu-pytorch/wheels/torch_xla-1.9-cp37-cp37m-linux_x86_64.whl (149.9 MB)\n",
            "\u001b[K     |████████████████████████████████| 149.9 MB 50 kB/s \n",
            "\u001b[?25hCollecting cloud-tpu-client==0.10\n",
            "  Downloading cloud_tpu_client-0.10-py3-none-any.whl (7.4 kB)\n",
            "Requirement already satisfied: oauth2client in /usr/local/lib/python3.7/dist-packages (from cloud-tpu-client==0.10) (4.1.3)\n",
            "Collecting google-api-python-client==1.8.0\n",
            "  Downloading google_api_python_client-1.8.0-py3-none-any.whl (57 kB)\n",
            "\u001b[K     |████████████████████████████████| 57 kB 2.3 MB/s \n",
            "\u001b[?25hRequirement already satisfied: httplib2<1dev,>=0.9.2 in /usr/local/lib/python3.7/dist-packages (from google-api-python-client==1.8.0->cloud-tpu-client==0.10) (0.17.4)\n",
            "Requirement already satisfied: google-auth>=1.4.1 in /usr/local/lib/python3.7/dist-packages (from google-api-python-client==1.8.0->cloud-tpu-client==0.10) (1.32.1)\n",
            "Requirement already satisfied: google-api-core<2dev,>=1.13.0 in /usr/local/lib/python3.7/dist-packages (from google-api-python-client==1.8.0->cloud-tpu-client==0.10) (1.26.3)\n",
            "Requirement already satisfied: uritemplate<4dev,>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from google-api-python-client==1.8.0->cloud-tpu-client==0.10) (3.0.1)\n",
            "Requirement already satisfied: six<2dev,>=1.6.1 in /usr/local/lib/python3.7/dist-packages (from google-api-python-client==1.8.0->cloud-tpu-client==0.10) (1.15.0)\n",
            "Requirement already satisfied: google-auth-httplib2>=0.0.3 in /usr/local/lib/python3.7/dist-packages (from google-api-python-client==1.8.0->cloud-tpu-client==0.10) (0.0.4)\n",
            "Requirement already satisfied: googleapis-common-protos<2.0dev,>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from google-api-core<2dev,>=1.13.0->google-api-python-client==1.8.0->cloud-tpu-client==0.10) (1.53.0)\n",
            "Requirement already satisfied: pytz in /usr/local/lib/python3.7/dist-packages (from google-api-core<2dev,>=1.13.0->google-api-python-client==1.8.0->cloud-tpu-client==0.10) (2018.9)\n",
            "Requirement already satisfied: protobuf>=3.12.0 in /usr/local/lib/python3.7/dist-packages (from google-api-core<2dev,>=1.13.0->google-api-python-client==1.8.0->cloud-tpu-client==0.10) (3.17.3)\n",
            "Requirement already satisfied: requests<3.0.0dev,>=2.18.0 in /usr/local/lib/python3.7/dist-packages (from google-api-core<2dev,>=1.13.0->google-api-python-client==1.8.0->cloud-tpu-client==0.10) (2.23.0)\n",
            "Requirement already satisfied: setuptools>=40.3.0 in /usr/local/lib/python3.7/dist-packages (from google-api-core<2dev,>=1.13.0->google-api-python-client==1.8.0->cloud-tpu-client==0.10) (57.2.0)\n",
            "Requirement already satisfied: packaging>=14.3 in /usr/local/lib/python3.7/dist-packages (from google-api-core<2dev,>=1.13.0->google-api-python-client==1.8.0->cloud-tpu-client==0.10) (21.0)\n",
            "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from google-auth>=1.4.1->google-api-python-client==1.8.0->cloud-tpu-client==0.10) (4.2.2)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from google-auth>=1.4.1->google-api-python-client==1.8.0->cloud-tpu-client==0.10) (0.2.8)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.7/dist-packages (from google-auth>=1.4.1->google-api-python-client==1.8.0->cloud-tpu-client==0.10) (4.7.2)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=14.3->google-api-core<2dev,>=1.13.0->google-api-python-client==1.8.0->cloud-tpu-client==0.10) (2.4.7)\n",
            "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.7/dist-packages (from pyasn1-modules>=0.2.1->google-auth>=1.4.1->google-api-python-client==1.8.0->cloud-tpu-client==0.10) (0.4.8)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0dev,>=2.18.0->google-api-core<2dev,>=1.13.0->google-api-python-client==1.8.0->cloud-tpu-client==0.10) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0dev,>=2.18.0->google-api-core<2dev,>=1.13.0->google-api-python-client==1.8.0->cloud-tpu-client==0.10) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0dev,>=2.18.0->google-api-core<2dev,>=1.13.0->google-api-python-client==1.8.0->cloud-tpu-client==0.10) (2021.5.30)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0dev,>=2.18.0->google-api-core<2dev,>=1.13.0->google-api-python-client==1.8.0->cloud-tpu-client==0.10) (3.0.4)\n",
            "Installing collected packages: google-api-python-client, torch-xla, cloud-tpu-client\n",
            "  Attempting uninstall: google-api-python-client\n",
            "    Found existing installation: google-api-python-client 1.12.8\n",
            "    Uninstalling google-api-python-client-1.12.8:\n",
            "      Successfully uninstalled google-api-python-client-1.12.8\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "earthengine-api 0.1.272 requires google-api-python-client<2,>=1.12.1, but you have google-api-python-client 1.8.0 which is incompatible.\u001b[0m\n",
            "Successfully installed cloud-tpu-client-0.10 google-api-python-client-1.8.0 torch-xla-1.9\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:root:Waiting for TPU to be start up with version pytorch-1.9...\n",
            "WARNING:root:Waiting for TPU to be start up with version pytorch-1.9...\n",
            "WARNING:root:TPU has started up successfully with version pytorch-1.9\n"
          ]
        }
      ],
      "metadata": {
        "id": "rvd7G4AX8JuZ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "422897bd-55e9-41e0-8577-b31b3d8e10bc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Download Data\n",
        "\n"
      ],
      "metadata": {
        "id": "U46JvI7eZIRy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "To download the Kaggle dataset, we must first mount our Google Drive to this Colab notebook."
      ],
      "metadata": {
        "id": "VZAD6BgU4_C5"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "drive.mount('/content/drive')"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "metadata": {
        "id": "4JlkraYu3eU_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ecd7a16c-9a1e-418c-84fa-9035620c14c1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Then, we specify the config path to our Kaggle API token (in the form of a `kaggle.json` file), and change the current working directory to that path."
      ],
      "metadata": {
        "id": "_iCah6A_5Mgu"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "os.environ['KAGGLE_CONFIG_DIR'] = '/content/drive/MyDrive/Research/Dynamic Spectra Sequence Modeling/Data/Kaggle'\n",
        "%cd '/content/drive/MyDrive/Research/Dynamic Spectra Sequence Modeling/Data'"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive/Research/Transformers/Code/Data\n"
          ]
        }
      ],
      "metadata": {
        "id": "bVzYMI8C3qX3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "936bb8c7-b567-4e47-808a-85e53471f22d"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Finally, we copy and run the API command for the BL Kaggle competition to download the datasets (remember to unzip the files)."
      ],
      "metadata": {
        "id": "OUrCQWsk5e3z"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "if not os.listdir():\n",
        "  # Note, if you're getting the error message \"429 - Too Many Requests\", try running the following commands before the API command:\n",
        "  # !pip uninstall -y kaggle\n",
        "  # !pip install --upgrade pip\n",
        "  # !pip install kaggle==1.5.6\n",
        "  !kaggle competitions download -c seti-breakthrough-listen\n",
        "  !unzip seti-breakthrough-listen.zip"
      ],
      "outputs": [],
      "metadata": {
        "id": "DVr3S-zQ4cZL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Prep Data"
      ],
      "metadata": {
        "id": "Cm-0buX-OPfS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We want to create lookup tables in the form of Python dictionaries, with ID-target key-value pairs, for both the training and test data. \n",
        "\n",
        "To do so for the training data is quite straightforward. Note however, that the test data IDs have been hashed for security purposes, hence we must go through some extra steps beforehand."
      ],
      "metadata": {
        "id": "LNjADlBANJKt"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "train_labels = read_csv('train_labels.csv')\n",
        "train_dict = dict(zip(train_labels.id, train_labels.target))\n",
        "\n",
        "original_labels = read_csv('sample_submission.csv')['id']\n",
        "hash_labels = read_csv('masked_labels.csv')\n",
        "test_dict = {}\n",
        "keyword = input('Enter keyword: ')\n",
        "for labels in tqdm(original_labels):\n",
        "  m = hashlib.md5(keyword.encode(\"utf-8\"))\n",
        "  m.update(bytes.fromhex(\"0\" + labels))\n",
        "  hashed_id = m.hexdigest()\n",
        "  test_dict[labels] = hash_labels.loc[hash_labels['id'] == hashed_id, 'target'].item()"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Enter keyword: zach\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 39995/39995 [01:51<00:00, 359.27it/s]\n"
          ]
        }
      ],
      "metadata": {
        "id": "LwbuZ_HIMAEH",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "48103229-7a15-4d93-d789-c411ba7e894f"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Split the training set into non-overlapping new datasets for cross-validation. Note that `x_train` and `x_valid` will hold the ID values, whereas `y_train` and `y_valid` will hold the target values (both with lengths `(48000, 12000)`, respectively). Since our model is self-supervised, we'll only use `y_train` and `y_valid` for validation using downstream tasks."
      ],
      "metadata": {
        "id": "94bgHuHh2fJA"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "len_train = int(len(train_labels) * 0.8)\n",
        "len_valid = int(len(train_labels) * 0.2)\n",
        "\n",
        "x_train, x_valid = random_split(train_labels['id'], (len_train, len_valid))\n",
        "y_train, y_valid = random_split(train_labels['target'], (len_train, len_valid))"
      ],
      "outputs": [],
      "metadata": {
        "id": "ImXsAW2U5Ogn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Train"
      ],
      "metadata": {
        "id": "ef0v8XnaMHqD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Initialize the random seed."
      ],
      "metadata": {
        "id": "wgrulI_I1cMu"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "# Random Seed Initialize\n",
        "RANDOM_SEED = 11\n",
        "def seed_everything(seed=RANDOM_SEED):\n",
        "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
        "    np.random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed(seed)\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "    torch.backends.cudnn.benchmark = True\n",
        "seed_everything()"
      ],
      "outputs": [],
      "metadata": {
        "id": "d8t1knbZ1bf5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Set the model parameters."
      ],
      "metadata": {
        "id": "8VZ2xfHx9ms2"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "checkpoint_path = '../Code/Checkpoints/.'\n",
        "num_cores = 8\n",
        "num_workers = 0\n",
        "epochs = 30\n",
        "batch_size = 128\n",
        "learning_rate = 1e-4\n",
        "\n",
        "\n",
        "d_model = 256   # Latent dim\n",
        "d_input = (6, t, 256)   # Input dim (from dataset), where 0<= t <= 6*273\n",
        "d_output = (6, 1, 256)   # Output dim (from dataset)\n",
        "q = 8   # Query size\n",
        "v = 8   # Value size\n",
        "h = 8   # Number of heads\n",
        "N = 4   # Number of decoder blocks to stack\n",
        "attention_size = 12   # Attention window size\n",
        "dropout = 0.2   # Dropout rate\n",
        "pe = None\n",
        "chunk_mode = None\n",
        "\n",
        "# training_params = {\n",
        "#     'checkpoint_path': '../Checkpoints/.',\n",
        "#     'num_cores': 8,\n",
        "#     'num_workers': 0,\n",
        "#     'epochs': 30,\n",
        "#     'batch_size': 128,\n",
        "#     'learning_rate': 1e-4\n",
        "# }\n",
        "\n",
        "# # Dimensions for data are (6, 273, 256), i.e. 6 snippets of 273 timesteps and 256 frequency channels\n",
        "# model_params = {\n",
        "#     'd_model': 256, # Latent dim\n",
        "#     'd_input': (6, t, 256), # Input dim (from dataset), where 0<= t <= 6*273\n",
        "#     'd_output': (6, 1, 256), # Output dim (from dataset)\n",
        "#     'q': 8, # Query size\n",
        "#     'v': 8, # Value size\n",
        "#     'h': 8, # Number of heads\n",
        "#     'N': 4, # Number of decoder blocks to stack\n",
        "#     'attention_size': 12, # Attention window size\n",
        "#     'dropout': 0.2, # Dropout rate\n",
        "#     'pe': None,\n",
        "#     'chunk_mode': None\n",
        "# }"
      ],
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-7e936f49be0a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m training_params = {\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0;34m'device'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mxm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mxla_device\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m     \u001b[0;34m'lr'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;36m1e-4\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0;34m'batch_size'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;36m1024\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;31m# 128 * 8 - batch size of 128 for each of the 8 TPU cores (to avoid bottlenecking)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0;34m'epochs'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;36m30\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'xm' is not defined"
          ]
        }
      ],
      "metadata": {
        "id": "I8H8YOUr9n-y",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 232
        },
        "collapsed": true,
        "outputId": "0b86d898-b634-4955-c30c-1e05089fdaf9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Configuring Colab's Cloud TPUs\n",
        "\n"
      ],
      "metadata": {
        "id": "THxIEOk7KhFw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Colab provides a free Cloud TPU system (a remote CPU host + four TPU chips with two cores each). To gain access to a TPU on Colab, on the main menu, click Runtime > Change runtime type > set \"TPU\" as the hardware accelerator.\n",
        "\n",
        "The PyTorch/XLA package lets PyTorch connect to Cloud TPUs (It's named PyTorch/XLA, not PyTorch/TPU, because XLA is the name of the TPU compiler), and makes TPU cores available as PyTorch devices, which lets PyTorch create and manipulate tensors on TPUs."
      ],
      "metadata": {
        "id": "Hiys2kfbXulk"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "assert os.environ['COLAB_TPU_ADDR'], 'Make sure to select TPU from Edit > Notebook settings > Hardware accelerator'"
      ],
      "outputs": [],
      "metadata": {
        "id": "vTOMNye1KjUb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "`torch.utils.data.distributed.DistrubutedSampler()` distributes the training data evenly (with no replicas) to all 8 TPU cores that Colab provides. Note that `xm.xrt_world_size()` retrieves the number of devices that are taking part in the replication (basically the number of cores), and `xm.get_ordinal()` retrieves the replication ordinal of the current process. The ordinals range from `0` to `xrt_world_size()-1`."
      ],
      "metadata": {
        "id": "GqCXmZ6d-7C3"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "train_sampler = DistributedSampler(\n",
        "    x_train,\n",
        "    num_replicas=xm.xrt_world_size(),\n",
        "    rank=xm.get_ordinal(),\n",
        "    shuffle=True)\n",
        "     \n",
        "valid_sampler = DistributedSampler(\n",
        "    x_valid,\n",
        "    num_replicas=xm.xrt_world_size(),\n",
        "    rank=xm.get_ordinal(),\n",
        "    shuffle=False)"
      ],
      "outputs": [],
      "metadata": {
        "id": "Xt2MK5_E_O0w"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "After the data has been distributed, we can create dataloaders using `ParallelLoader`."
      ],
      "metadata": {
        "id": "bAggrSznGj8B"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "train_loader = DataLoader(\n",
        "    x_train,\n",
        "    batch_size=training_params['batch_size'],\n",
        "    sampler=train_sampler,\n",
        "    num_workers=training_params['num_workers'],\n",
        "    shuffle=True,\n",
        "    drop_last=True)\n",
        "\n",
        "valid_loader = DataLoader(\n",
        "    x_valid,\n",
        "    batch_size=training_params['batch_size'],\n",
        "    sampler=train_sampler,\n",
        "    num_workers=training_params['num_workers'],\n",
        "    shuffle=False,\n",
        "    drop_last=True)\n",
        "\n",
        "# drop_last = True drops the last incomplete batch if the dataset size is not divisible by the batch size\n",
        "# drop_last = False will cause the last batch to be smaller if the size of dataset is not divisible by the batch size"
      ],
      "outputs": [],
      "metadata": {
        "id": "z0PmQAFsG5pw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Optimize parameters for distributed training on TPU cores (remember `xm.xrt_world_size()` returns the number of TPU cores, which for our case is 8)."
      ],
      "metadata": {
        "id": "Mrtz5gFCojZi"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "# Scale learning rate to world size\n",
        "lr = training_params['learning_rate'] * xm.xrt_world_size()\n",
        "\n",
        "# Get loss function, optimizer, and model\n",
        "device = xm.xla_device()\n",
        "model = Transformer().to(device)\n",
        "optimizer = optim.Adam(model.parameters(), lr=lr)\n",
        "loss_function = OZELoss(alpha=0.3)"
      ],
      "outputs": [],
      "metadata": {
        "id": "X5qTozz1omvK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Loop"
      ],
      "metadata": {
        "id": "EygLIhmH5rP6"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [],
      "outputs": [],
      "metadata": {
        "id": "fCZd2HcK5ud8"
      }
    }
  ]
}